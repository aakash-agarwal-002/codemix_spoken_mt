{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aabc211a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ssl\n",
    "import zipfile\n",
    "import urllib.request\n",
    "from datasets import Dataset, DatasetDict, load_from_disk, load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38665c13",
   "metadata": {},
   "source": [
    "#### IITB Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d3f0aae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load HuggingFace dataset from disk...\n",
      "Loaded existing dataset.\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['english', 'hindi'],\n",
      "        num_rows: 1161358\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['english', 'hindi'],\n",
      "        num_rows: 165908\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['english', 'hindi'],\n",
      "        num_rows: 331817\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.cfilt.iitb.ac.in/iitb_parallel/iitb_corpus_download/parallel.zip\"\n",
    "data_dir = \"./datasets/IITB\"\n",
    "zip_path = os.path.join(data_dir, \"parallel.zip\")\n",
    "# 1 Try loading existing HuggingFace dataset firs\n",
    "try:\n",
    "    print(\"Trying to load HuggingFace dataset from disk...\")\n",
    "    dataset = load_from_disk(data_dir)\n",
    "    print(\"Loaded existing dataset.\")\n",
    "    \n",
    "except Exception:\n",
    "    print(\"No saved HuggingFace dataset found. Building from raw files...\")\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    # 2 Download only if zip not present\n",
    "    if not os.path.exists(zip_path):\n",
    "        print(\"Downloading dataset...\")\n",
    "        ssl_context = ssl._create_unverified_context()\n",
    "        with urllib.request.urlopen(url, context=ssl_context) as response, \\\n",
    "             open(zip_path, \"wb\") as out_file:\n",
    "            out_file.write(response.read())\n",
    "\n",
    "    # 3 Extract only if not already extracted\n",
    "    extracted_marker = os.path.join(data_dir, \"parallel\")\n",
    "    if not os.path.exists(extracted_marker):\n",
    "        print(\"Extracting dataset...\")\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(data_dir)\n",
    "\n",
    "    # 4 Automatically locate .en and .hi files\n",
    "    en_file = None\n",
    "    hi_file = None\n",
    "\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".en\") and \"IITB\" in file:\n",
    "                en_file = os.path.join(root, file)\n",
    "            if file.endswith(\".hi\") and \"IITB\" in file:\n",
    "                hi_file = os.path.join(root, file)\n",
    "\n",
    "    if en_file is None or hi_file is None:\n",
    "        raise FileNotFoundError(\"Could not locate IITB .en and .hi files.\")\n",
    "\n",
    "    print(\"Using files:\")\n",
    "    print(en_file)\n",
    "    print(hi_file)\n",
    "\n",
    "    # 5 Load corpus\n",
    "    en_sentences = []\n",
    "    hi_sentences = []\n",
    "\n",
    "    with open(en_file, \"r\", encoding=\"utf-8\") as f_en, \\\n",
    "         open(hi_file, \"r\", encoding=\"utf-8\") as f_hi:\n",
    "        for en, hi in zip(f_en, f_hi):\n",
    "            en_sentences.append(en.strip())\n",
    "            hi_sentences.append(hi.strip())\n",
    "\n",
    "    full_dataset = Dataset.from_dict({\n",
    "        \"english\": en_sentences,\n",
    "        \"hindi\": hi_sentences\n",
    "    })\n",
    "\n",
    "    # 6 70/10/20 split\n",
    "    split_1 = full_dataset.train_test_split(test_size=0.30, seed=42)\n",
    "    train_dataset = split_1[\"train\"]\n",
    "    temp_dataset = split_1[\"test\"]\n",
    "\n",
    "    split_2 = temp_dataset.train_test_split(test_size=2/3, seed=42)\n",
    "\n",
    "    dataset = DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"validation\": split_2[\"train\"],\n",
    "        \"test\": split_2[\"test\"]\n",
    "    })\n",
    "\n",
    "    dataset.save_to_disk(data_dir)\n",
    "    print(\"Saved HuggingFace dataset to disk.\")\n",
    "# Final output\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3c993869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'english': 'The British at once agreed and also made him their agent.',\n",
       " 'hindi': 'जैसे समय बीतता गया परिस्थितियों में भी सुधार होता गया।'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29869362",
   "metadata": {},
   "source": [
    "#### Open Subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a43c0c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing dataset.\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['english', 'hindi'],\n",
      "        num_rows: 2103175\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['english', 'hindi'],\n",
      "        num_rows: 300454\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['english', 'hindi'],\n",
      "        num_rows: 600908\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "url = \"https://object.pouta.csc.fi/OPUS-OpenSubtitles/v2024/moses/en-hi.txt.zip\"\n",
    "data_dir = \"./datasets/OpenSubtitles\"\n",
    "zip_path = os.path.join(data_dir, \"en-hi.txt.zip\")\n",
    "\n",
    "# try loading existing HuggingFace dataset\n",
    "try:\n",
    "    dataset = load_from_disk(data_dir)\n",
    "    print(\"Loaded existing dataset.\")\n",
    "\n",
    "except Exception:\n",
    "    print(\"Building dataset from raw files...\")\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    # download if zip not present\n",
    "    if not os.path.exists(zip_path):\n",
    "        ssl_context = ssl._create_unverified_context()\n",
    "        with urllib.request.urlopen(url, context=ssl_context) as response, \\\n",
    "             open(zip_path, \"wb\") as out_file:\n",
    "            out_file.write(response.read())\n",
    "\n",
    "    # extract if not already extracted\n",
    "    extract_marker = os.path.join(data_dir, \"OpenSubtitles.en-hi.en\")\n",
    "    if not os.path.exists(extract_marker):\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(data_dir)\n",
    "\n",
    "    # find .en and .hi files\n",
    "    en_file = None\n",
    "    hi_file = None\n",
    "\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".en\"):\n",
    "                en_file = os.path.join(root, file)\n",
    "            if file.endswith(\".hi\"):\n",
    "                hi_file = os.path.join(root, file)\n",
    "\n",
    "    if en_file is None or hi_file is None:\n",
    "        raise FileNotFoundError(\"Could not locate .en and .hi files.\")\n",
    "\n",
    "    # load corpus\n",
    "    en_sentences = []\n",
    "    hi_sentences = []\n",
    "\n",
    "    with open(en_file, \"r\", encoding=\"utf-8\") as f_en, \\\n",
    "         open(hi_file, \"r\", encoding=\"utf-8\") as f_hi:\n",
    "        for en, hi in zip(f_en, f_hi):\n",
    "            en_sentences.append(en.strip())\n",
    "            hi_sentences.append(hi.strip())\n",
    "\n",
    "    full_dataset = Dataset.from_dict({\n",
    "        \"english\": en_sentences,\n",
    "        \"hindi\": hi_sentences\n",
    "    })\n",
    "\n",
    "    # 70/10/20 split\n",
    "    split_1 = full_dataset.train_test_split(test_size=0.30, seed=42)\n",
    "    train_dataset = split_1[\"train\"]\n",
    "    temp_dataset = split_1[\"test\"]\n",
    "\n",
    "    split_2 = temp_dataset.train_test_split(test_size=2/3, seed=42)\n",
    "\n",
    "    dataset = DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"validation\": split_2[\"train\"],\n",
    "        \"test\": split_2[\"test\"]\n",
    "    })\n",
    "\n",
    "    dataset.save_to_disk(data_dir)\n",
    "    print(\"Saved dataset.\")\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8482bdcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'english': ['Look at me.',\n",
       "  \"So, I'm gonna get back to work.\",\n",
       "  \"I guess there is some small comfort in knowing that either way, we're dead.\",\n",
       "  'They determined that three seconds of eye contact made the other person feel most comfortable.',\n",
       "  'I never did the work to find out who I am or who I am supposed to be.',\n",
       "  \"He actually didn't deliver the car to Gol Bandya... but to Fayyaz bhaee, the Don.\",\n",
       "  \"I'm the Chief of Communications.\",\n",
       "  \"Where's my cardigan?\",\n",
       "  '\"Where\\'s the dust?',\n",
       "  \"I've been hearing you come home every night at God knows what hour.\",\n",
       "  'With pretty names',\n",
       "  'But that means that Mother Ginger has it.',\n",
       "  'Why?',\n",
       "  '- Hey, get the fuck off me!',\n",
       "  \"We'll go over here, you stay there. Don't listen to what we're saying.\",\n",
       "  'Can I have a picture please?',\n",
       "  \"Let's utilize the mindset of unlimited potential to discuss this new business venture the boss man wants to talk about.\",\n",
       "  'I do not know.',\n",
       "  \"'Milk is flowing nonstop!\",\n",
       "  \"You're going crazy and don't want to admit it.\",\n",
       "  'Louis was like, \"No, I need to see options.\"',\n",
       "  'Works perfectly.',\n",
       "  'Where is my money, Robert?',\n",
       "  '- Mom.',\n",
       "  'The doorbell works.',\n",
       "  \"You're the one who yelled last night?\",\n",
       "  'What should we do after?',\n",
       "  \"- Sit down, I'm buying you dinner.\",\n",
       "  '-you kidding? -Did she tell you that?',\n",
       "  'Oh, yeah.',\n",
       "  \"You're an inefficient Police Officer who couldn't crack Rudra's case.\",\n",
       "  \"Even though I'm still learning about this Protector stuff,\",\n",
       "  \"Where's my uncle?\",\n",
       "  \"Someone's here.\",\n",
       "  'Tails.',\n",
       "  \"There's no room.\",\n",
       "  \"You don't have to shout.\",\n",
       "  \"I've been going over every picture, every line, every word, every number.\",\n",
       "  'Just stay with me and Rosa, we get it.',\n",
       "  \"Like dog's greed is gone by showing a stone\",\n",
       "  \"♪ My dear, I fear that we're kaput ♪ [dancers] ♪ Kaput ♪\",\n",
       "  'Goodbye, old friend!',\n",
       "  'Let her be self-reliant.',\n",
       "  \"Penny said she'll know more in a week.\",\n",
       "  \"That's what I thought.\",\n",
       "  \"Yeah, we work together, but you don't know me, and I don't know you, and I'm perfectly fine with that arrangement.\",\n",
       "  '[Red Team member shouting in reverse]',\n",
       "  'Oh, shit, you are sweet!',\n",
       "  'Are you from one of those creepy doomsday prep families and have a basement full of canned soup?'],\n",
       " 'hindi': ['मुझे देख।',\n",
       "  'इसलए,मैंवपसकमपर जरह हूँ।',\n",
       "  'शयदइसबत कजनकरथ ड़सुकूनमलतहै कद नंकसूरतं में,हम मरेंगे।',\n",
       "  'उन्हें पतचलकतन सेकंडतकनज़रेंमल नेसे समनेव लेकअच्छ लगतहै ।',\n",
       "  'मैंने कभयहतल शकरनेक कशशहनहंक कमैं कन हूँ यमुझेक्यहनचह ए।',\n",
       "  'lrm; उसने गड़गल बंड्यकनहं, lrm;',\n",
       "  '\\u200eमैं संचरव भगकप्रमुखहूँ।',\n",
       "  'मेरक र्डगनकहँ है?',\n",
       "  '\"धूल कहँहै?',\n",
       "  'मैं सुन रहहूँकतुम बहुत देर से रतक घर पर आतेह।',\n",
       "  '♪',\n",
       "  'लेकनइसकमतलबहैक मँअदरकहै ।',\n",
       "  'क्यूं कर?',\n",
       "  '-[नशे में चख]हट ,मदरचद,अपनहथ हट!',\n",
       "  '\\u200eहम उधर जतेहैंऔर तुमधररहन।',\n",
       "  'क्यएकतस्वरलेसकतहूँ?',\n",
       "  'चलए,यहबड़ेसहब जसनए कमक बत करन चहतेहैं, उस पर चर्चकरनेके लए हम असमतक्षमतकमनस कत कप्रयगकरें।',\n",
       "  '-मुझे नहंपत।',\n",
       "  'दूध तलगतरबहरह है !',\n",
       "  'तुम पगलह रहेह और तुम मनननह ंचहते।',\n",
       "  'लुइस ने कह,\" नहं,मुझेवकल्पदेखने हैं।\"',\n",
       "  'सहक मकरतहै।',\n",
       "  'मेरपैसकहँ है,र बर्ट?',\n",
       "  '-मँ।',\n",
       "  'घंटबजतहै।',\n",
       "  '\\u200eतुम्हंच ल्लएथेकलरत ?',\n",
       "  'उसके बदहमक्यकरेंगे?',\n",
       "  'बैठ जओ,आजकड नर मेरतरफ़से।',\n",
       "  '-उसने तुमसे कह?',\n",
       "  'अरे, हँ।',\n",
       "  'आप एक अक्षम पुलसअधकरहैंजरुद्र के ममलेक तड़नहंसके।',\n",
       "  'lrm; मैं संरक्षक हनेकेबरेमें lrm; भले हस खरह हूँ ,',\n",
       "  'मेरच चकहँ है?',\n",
       "  \"Someone's here.\",\n",
       "  'पट।',\n",
       "  '\\u202aअरे, आज भड़है।',\n",
       "  'चल्लनेकज़रूरतनहं।',\n",
       "  'मैं हर चत्र,हरपंक्त, हर शब्द, हर नंबर कध्यनसेज ँच रहहूँ।',\n",
       "  'बस मेरे सथरहऔरर ज,  हम समझ गए।',\n",
       "  'कुत्ते के लभक तरहएकपत्थरदखकरचलगयहै',\n",
       "  'मेरज न,मुझेअफ़ससहै हमरकह नख़त्महचुकहै कहनख़त्म',\n",
       "  'क्यंनहं? \"सब कुछ संभव है।',\n",
       "  'भभडल कआत्मन र्भरबननेदे न।',\n",
       "  'पेननेकह कउसेएक हफ्तेमेंऔरपत  चलेग।',\n",
       "  'ब ल्कुलयह मैनेसच।',\n",
       "  'हँ,हमसथ कमकरतेहैं, पर तुम मुझे नहंज नते, और मैं तुम्हें नहंज नत। और मुझे इससे कईएतरज़नहं ।',\n",
       "  '[रेड टमक सदस्य उल्टच ल्लरहहै]',\n",
       "  'Oh, shit, you are sweet!',\n",
       "  'क्यतुमउन कय मतक तैयरकरनेवले परवर ंसेह जतहखनेमें सूप केड ब्बे जमकरकेरखतेहैं ?']}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][1:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8db5a09",
   "metadata": {},
   "source": [
    "#### Ted talks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "29df33d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing dataset.\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['english', 'hindi'],\n",
      "        num_rows: 33343\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['english', 'hindi'],\n",
      "        num_rows: 4763\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['english', 'hindi'],\n",
      "        num_rows: 9527\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "url = \"https://object.pouta.csc.fi/OPUS-TED2020/v1/moses/en-hi.txt.zip\"\n",
    "data_dir = \"./datasets/TED2020\"\n",
    "zip_path = os.path.join(data_dir, \"en-hi.txt.zip\")\n",
    "\n",
    "# try loading existing HuggingFace dataset\n",
    "try:\n",
    "    dataset = load_from_disk(data_dir)\n",
    "    print(\"Loaded existing dataset.\")\n",
    "\n",
    "except Exception:\n",
    "    print(\"Building dataset from raw files...\")\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    # download if zip not present\n",
    "    if not os.path.exists(zip_path):\n",
    "        ssl_context = ssl._create_unverified_context()\n",
    "        with urllib.request.urlopen(url, context=ssl_context) as response, \\\n",
    "             open(zip_path, \"wb\") as out_file:\n",
    "            out_file.write(response.read())\n",
    "\n",
    "    # extract if not already extracted\n",
    "    extract_marker = os.path.join(data_dir, \"TED2020.en-hi.en\")\n",
    "    if not os.path.exists(extract_marker):\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(data_dir)\n",
    "\n",
    "    # find .en and .hi files\n",
    "    en_file = None\n",
    "    hi_file = None\n",
    "\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".en\"):\n",
    "                en_file = os.path.join(root, file)\n",
    "            if file.endswith(\".hi\"):\n",
    "                hi_file = os.path.join(root, file)\n",
    "\n",
    "    if en_file is None or hi_file is None:\n",
    "        raise FileNotFoundError(\"Could not locate .en and .hi files.\")\n",
    "\n",
    "    # load corpus\n",
    "    en_sentences = []\n",
    "    hi_sentences = []\n",
    "\n",
    "    with open(en_file, \"r\", encoding=\"utf-8\") as f_en, \\\n",
    "         open(hi_file, \"r\", encoding=\"utf-8\") as f_hi:\n",
    "        for en, hi in zip(f_en, f_hi):\n",
    "            en_sentences.append(en.strip())\n",
    "            hi_sentences.append(hi.strip())\n",
    "\n",
    "    full_dataset = Dataset.from_dict({\n",
    "        \"english\": en_sentences,\n",
    "        \"hindi\": hi_sentences\n",
    "    })\n",
    "\n",
    "    # 70/10/20 split\n",
    "    split_1 = full_dataset.train_test_split(test_size=0.30, seed=42)\n",
    "    train_dataset = split_1[\"train\"]\n",
    "    temp_dataset = split_1[\"test\"]\n",
    "\n",
    "    split_2 = temp_dataset.train_test_split(test_size=2/3, seed=42)\n",
    "\n",
    "    dataset = DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"validation\": split_2[\"train\"],\n",
    "        \"test\": split_2[\"test\"]\n",
    "    })\n",
    "\n",
    "    dataset.save_to_disk(data_dir)\n",
    "    print(\"Saved dataset.\")\n",
    "\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5dc3ce38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'english': ['At home, online, in school, in their communities.',\n",
       "  \"And that tells us something, that this isn't entertainment for children anymore.\",\n",
       "  \"The final thing is the notion of India as a single market -- because when you didn't think of India as a market, you didn't really bother about a single market, because it didn't really matter.\",\n",
       "  'We had Jim Dombrowski, Albert Ben Smith, who started all kinds of things right in that restaurant, and nobody ever bothered us.',\n",
       "  \"We're using cognition to control our behavior.\",\n",
       "  'Myself, I will go back to the East.',\n",
       "  \"But it isn't that I'm not grateful, but I think, as long as you're living, you've got to keep moving, you've got to keep trying to get up and do what you've got to do.\",\n",
       "  'What will you do with your intentional empty space, with your fresh start?',\n",
       "  \"They don't know, OK, and they're trying to get another member of The 99 to join them.\"],\n",
       " 'hindi': ['घर में, ऑनलाइन, स्कूल में, अपने समुदायों में ।',\n",
       "  'और वह हमें कुछ बताता है, यह मनोरंजन नहीं है बच्चों के लिए अब और नहीं।',\n",
       "  'अंतिम बात, एकल बाजार के रूप में भारत को देखना है - क्योंकि जब आप भारत को एक बाजार के रूप में नहीं देखते, तुम आप एक एकल बाजार के बारे में परेशान नहीं थे, क्योंकि इससे फर्क नहीं पड़ता था.',\n",
       "  'हमारे रेस्तराँ में जिम डमब्रॉव्सकी, एलबर्ट बेन स्मिथ ने हर कुछ शुरू किया, और किसीने हमें कभी तंग नहीं किया।',\n",
       "  'हम यह ज्ञान का इस्तेमाल कर रहे हैं व्यवहार बदलने में|',\n",
       "  'में वापस पूर्व देश मे चला जाऊँगा|',\n",
       "  'पर ऐसा नहीं कि मैं कृतज्ञ नहीं हूँ, पर मुझे लगता है कि जब तक आप ज़िंदा हैं, आपको आगे बढ़ते रहना चाहिए, आपको उठने को कोशिश करते रहना चाहिए और जो काम करना है उसे करते रहना चाहिए।',\n",
       "  'आप क्या करेंगे अपनी जानी- बूझी खाली जगह के साथ, अपनी ताजा शुरुआत के साथ?',\n",
       "  \"उन्हें पता भी नही है, है न। और ये कोशिश कर रहे है कि 'द ९९' में से एक और उनसे जुड जाये।\"]}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854a700c",
   "metadata": {},
   "source": [
    "#### Samanantar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "26c360af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from local disk...\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['idx', 'english', 'hindi'],\n",
      "        num_rows: 7087994\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['idx', 'english', 'hindi'],\n",
      "        num_rows: 1012570\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['idx', 'english', 'hindi'],\n",
      "        num_rows: 2025142\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"./datasets/samanantar\"\n",
    "hf_marker = os.path.join(dataset_path, \"dataset_dict.json\")\n",
    "\n",
    "# if valid HF dataset exists → load\n",
    "if os.path.exists(hf_marker):\n",
    "    print(\"Loading dataset from local disk...\")\n",
    "    dataset = load_from_disk(dataset_path)\n",
    "\n",
    "else:\n",
    "    print(\"Dataset not found locally. Downloading...\")\n",
    "    os.makedirs(dataset_path, exist_ok=True)\n",
    "\n",
    "    dataset = load_dataset(\"ai4bharat/samanantar\", \"hi\")\n",
    "\n",
    "    # create 70/10/20 split if only train exists\n",
    "    if \"train\" in dataset and len(dataset) == 1:\n",
    "        full_dataset = dataset[\"train\"]\n",
    "\n",
    "        split_1 = full_dataset.train_test_split(test_size=0.30, seed=42)\n",
    "        temp_dataset = split_1[\"test\"]\n",
    "\n",
    "        split_2 = temp_dataset.train_test_split(test_size=2/3, seed=42)\n",
    "\n",
    "        dataset = DatasetDict({\n",
    "            \"train\": split_1[\"train\"],\n",
    "            \"validation\": split_2[\"train\"],\n",
    "            \"test\": split_2[\"test\"]\n",
    "        })\n",
    "\n",
    "    # rename columns if needed\n",
    "    for split in dataset.keys():\n",
    "        if \"src\" in dataset[split].column_names:\n",
    "            dataset[split] = dataset[split].rename_columns({\n",
    "                \"src\": \"english\",\n",
    "                \"tgt\": \"hindi\"\n",
    "            })\n",
    "\n",
    "    dataset.save_to_disk(dataset_path)\n",
    "    print(\"Saved dataset.\")\n",
    "\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eae78673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': 9583251,\n",
       " 'english': 'Modi is abusing history for political mileage',\n",
       " 'hindi': 'मोदी : राजनैतिक फायदे के लिये इतिहास का दुरूपयोग'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae9ac91",
   "metadata": {},
   "source": [
    "#### Global Voices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "535ab82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing dataset.\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['english', 'hindi'],\n",
      "        num_rows: 1843\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['english', 'hindi'],\n",
      "        num_rows: 263\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['english', 'hindi'],\n",
      "        num_rows: 528\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "url = \"https://object.pouta.csc.fi/OPUS-GlobalVoices/v2018q4/moses/en-hi.txt.zip\"\n",
    "data_dir = \"./datasets/GlobalVoices\"\n",
    "zip_path = os.path.join(data_dir, \"en-hi.txt.zip\")\n",
    "txt_path_en = os.path.join(data_dir, \"GlobalVoices.en-hi.en\")\n",
    "txt_path_hi = os.path.join(data_dir, \"GlobalVoices.en-hi.hi\")\n",
    "\n",
    "hf_marker = os.path.join(data_dir, \"dataset_dict.json\")\n",
    "\n",
    "# if valid HF dataset exists → load\n",
    "if os.path.exists(hf_marker):\n",
    "    print(\"Loaded existing dataset.\")\n",
    "    dataset = load_from_disk(data_dir)\n",
    "\n",
    "else:\n",
    "    print(\"Building dataset...\")\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    # download if needed\n",
    "    if not os.path.exists(zip_path):\n",
    "        print(\"Downloading dataset...\")\n",
    "        urllib.request.urlretrieve(url, zip_path)\n",
    "\n",
    "    # extract if needed\n",
    "    if not os.path.exists(txt_path_en) or not os.path.exists(txt_path_hi):\n",
    "        print(\"Extracting dataset...\")\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(data_dir)\n",
    "\n",
    "    # load corpus\n",
    "    en_sentences = []\n",
    "    hi_sentences = []\n",
    "\n",
    "    with open(txt_path_en, \"r\", encoding=\"utf-8\") as f_en, \\\n",
    "         open(txt_path_hi, \"r\", encoding=\"utf-8\") as f_hi:\n",
    "        for en, hi in zip(f_en, f_hi):\n",
    "            en_sentences.append(en.strip())\n",
    "            hi_sentences.append(hi.strip())\n",
    "\n",
    "    full_dataset = Dataset.from_dict({\n",
    "        \"english\": en_sentences,\n",
    "        \"hindi\": hi_sentences\n",
    "    })\n",
    "\n",
    "    # 70/10/20 split\n",
    "    split_1 = full_dataset.train_test_split(test_size=0.30, seed=42)\n",
    "    temp_dataset = split_1[\"test\"]\n",
    "\n",
    "    split_2 = temp_dataset.train_test_split(test_size=2/3, seed=42)\n",
    "\n",
    "    dataset = DatasetDict({\n",
    "        \"train\": split_1[\"train\"],\n",
    "        \"validation\": split_2[\"train\"],\n",
    "        \"test\": split_2[\"test\"]\n",
    "    })\n",
    "\n",
    "    dataset.save_to_disk(data_dir)\n",
    "    print(\"Saved dataset.\")\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0dd23936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'english': 'Videos showed policemen in plainclothes confronting the peaceful protesters, pulling and tearing the signs from Nga’s supporters.',\n",
       " 'hindi': 'कई वीडियो में सादे कपड़ों में पुलिसकर्मी को शांतिपूर्ण रूप से धरना दे रहे प्रदर्शनकारियों से तख्तियाँ को छीन कर फाड़ते देखा गया।'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
