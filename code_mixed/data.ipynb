{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4ecda96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset, load_from_disk, DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555da00b",
   "metadata": {},
   "source": [
    "### COMI-LINGUA\n",
    "- COMI-LINGUA (COde-MIxing and LINGuistic Insights on Natural Hinglish Usage and Annotation) is a high-quality Hindi-English code-mixed dataset, manually annotated by three annotators. It serves as a benchmark for multilingual NLP models by covering multiple foundational tasks.\n",
    "- Machine Translation (MT): Parallel translation of sentences in Romanized Hindi and Devanagari Hindi and English languages. Initial translation predictions were generated using the Llama 3.3 LLM, which annotators then refined and corrected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a4641830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from local disk...\n",
      "\n",
      "English similarity:\n",
      "1 vs 2: 0.9983840981273635\n",
      "1 vs 3: 0.9961508136951346\n",
      "2 vs 3: 0.9962505055414138\n",
      "\n",
      "Hinglish similarity:\n",
      "1 vs 2: 0.9813452195198115\n",
      "1 vs 3: 0.9728847464060675\n",
      "2 vs 3: 0.9748953744740767\n",
      "\n",
      "Columns after selection:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Sentences', 'Predicted_en_translation', 'Predicted_RH_translation', 'Predicted_DH_translation', 'english', 'hinglish', 'Annotator_1_DH_translation', 'Annotator_2_en_translation', 'annotator2_RH_translation', 'Annotator_2_DH_translation', 'Annotator_3_en_translation', 'annotator3_RH_translation', 'Annotator_3_DH_translation'],\n",
      "        num_rows: 19558\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['Sentences', 'Predicted_en_translation', 'Predicted_RH_translation', 'Predicted_DH_translation', 'english', 'hinglish', 'Annotator_1_DH_translation', 'Annotator_2_en_translation', 'annotator2_RH_translation', 'Annotator_2_DH_translation', 'Annotator_3_en_translation', 'annotator3_RH_translation', 'Annotator_3_DH_translation'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from difflib import SequenceMatcher\n",
    "import numpy as np\n",
    "\n",
    "dataset_name = \"LingoIITGN/COMI-LINGUA\"\n",
    "config_name = \"MT\"\n",
    "dataset_path = \"./datasets/comi_lingua_mt\"\n",
    "\n",
    "# load raw dataset as is\n",
    "if os.path.exists(os.path.join(dataset_path, \"dataset_dict.json\")):\n",
    "    print(\"Loading dataset from local disk...\")\n",
    "    dataset = load_from_disk(dataset_path)\n",
    "else:\n",
    "    print(\"Downloading dataset...\")\n",
    "    dataset = load_dataset(dataset_name, config_name)\n",
    "\n",
    "train_data = dataset[\"train\"]\n",
    "\n",
    "# similarity function\n",
    "def similarity(a, b):\n",
    "    return SequenceMatcher(None, str(a), str(b)).ratio()\n",
    "\n",
    "# compute average similarity\n",
    "def compute_similarity(col1, col2, sample_size=5000):\n",
    "    sims = []\n",
    "    for i in range(min(sample_size, len(train_data))):\n",
    "        sims.append(\n",
    "            similarity(train_data[i][col1], train_data[i][col2])\n",
    "        )\n",
    "    return np.mean(sims)\n",
    "\n",
    "# English similarity\n",
    "sim_en_1_2 = compute_similarity(\"Annotator_1_en_translation\",\n",
    "                                \"Annotator_2_en_translation\")\n",
    "\n",
    "sim_en_1_3 = compute_similarity(\"Annotator_1_en_translation\",\n",
    "                                \"Annotator_3_en_translation\")\n",
    "\n",
    "sim_en_2_3 = compute_similarity(\"Annotator_2_en_translation\",\n",
    "                                \"Annotator_3_en_translation\")\n",
    "\n",
    "# Hinglish similarity\n",
    "sim_rh_1_2 = compute_similarity(\"Annotator_1_RH_translation\",\n",
    "                                \"annotator2_RH_translation\")\n",
    "\n",
    "sim_rh_1_3 = compute_similarity(\"Annotator_1_RH_translation\",\n",
    "                                \"annotator3_RH_translation\")\n",
    "\n",
    "sim_rh_2_3 = compute_similarity(\"annotator2_RH_translation\",\n",
    "                                \"annotator3_RH_translation\")\n",
    "\n",
    "print(\"\\nEnglish similarity:\")\n",
    "print(\"1 vs 2:\", sim_en_1_2)\n",
    "print(\"1 vs 3:\", sim_en_1_3)\n",
    "print(\"2 vs 3:\", sim_en_2_3)\n",
    "\n",
    "print(\"\\nHinglish similarity:\")\n",
    "print(\"1 vs 2:\", sim_rh_1_2)\n",
    "print(\"1 vs 3:\", sim_rh_1_3)\n",
    "print(\"2 vs 3:\", sim_rh_2_3)\n",
    "\n",
    "# choose Annotator 1\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].rename_columns({\n",
    "        \"Annotator_1_en_translation\": \"english\",\n",
    "        \"Annotator_1_RH_translation\": \"hinglish\"\n",
    "    })\n",
    "\n",
    "print(\"\\nColumns after selection:\")\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39680754",
   "metadata": {},
   "source": [
    "- using Annotator_1_en_translation column as **English**\n",
    "- using Annotator_1_RH_translation column as **Hinglish**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8d4e39aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed dataset...\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['english', 'hinglish'],\n",
      "        num_rows: 17602\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['english', 'hinglish'],\n",
      "        num_rows: 1956\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['english', 'hinglish'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_from_disk, DatasetDict\n",
    "\n",
    "raw_dataset_path = \"./datasets/comi_lingua_mt\"\n",
    "processed_path = \"./datasets_final/comi_lingua_mt\"\n",
    "hf_marker = os.path.join(processed_path, \"dataset_dict.json\")\n",
    "\n",
    "# if processed dataset already exists → load\n",
    "if os.path.exists(hf_marker):\n",
    "    print(\"Loading processed dataset...\")\n",
    "    dataset = load_from_disk(processed_path)\n",
    "\n",
    "else:\n",
    "    print(\"Processed dataset not found. Building...\")\n",
    "\n",
    "    # load raw dataset\n",
    "    raw_dataset = load_from_disk(raw_dataset_path)\n",
    "\n",
    "    # split 10% of train as validation\n",
    "    split_train = raw_dataset[\"train\"].train_test_split(test_size=0.10, seed=42)\n",
    "\n",
    "    train_dataset = split_train[\"train\"]\n",
    "    val_dataset = split_train[\"test\"]\n",
    "    test_dataset = raw_dataset[\"test\"]\n",
    "\n",
    "    dataset = DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"validation\": val_dataset,\n",
    "        \"test\": test_dataset\n",
    "    })\n",
    "\n",
    "    # rename and keep selected columns\n",
    "    for split in dataset.keys():\n",
    "        dataset[split] = dataset[split].rename_columns({\n",
    "            \"Annotator_1_en_translation\": \"english\",\n",
    "            \"Annotator_1_RH_translation\": \"hinglish\"\n",
    "        })\n",
    "\n",
    "        dataset[split] = dataset[split].remove_columns(\n",
    "            [col for col in dataset[split].column_names\n",
    "             if col not in [\"english\", \"hinglish\"]]\n",
    "        )\n",
    "\n",
    "    dataset.save_to_disk(processed_path)\n",
    "    print(\"Saved processed dataset.\")\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ce069cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'english': 'Nawazuddin Siddiqui was recently seen at the 22nd Lions Gold Awards, where he was asked if you have any sorrow about this?',\n",
       " 'hinglish': 'Navaazuddin Siddiqui haal hi mein 22nd Lions Gold Awards mein nazar aaye, vahaan unse poochha gaya ki kya aapko is baat ka koi dukh nahin hai?'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2842d4",
   "metadata": {},
   "source": [
    "### PHINC\n",
    "PHINC (Parallel Hinglish Social Media Code-Mixed Corpus for Machine Translation): The dataset tackles challenges in translating noisy, informal, code-mixed social media text, offering 13,738 Hinglish-English sentence pairs manually annotated by 54 annotators for low-resource machine translation task.\n",
    "\n",
    "The dataset contains the following fields:\n",
    "- Hinglish Code-Mixed Sentence: The original sentence in Romanized Hindi-English (Hinglish).\n",
    "- Human Translated English Sentence: The corresponding English translation provided by human annotators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6f1aac38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from local disk...\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['hinglish', 'english', 'hinglish_dev'],\n",
      "        num_rows: 9616\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['hinglish', 'english', 'hinglish_dev'],\n",
      "        num_rows: 1374\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['hinglish', 'english', 'hinglish_dev'],\n",
      "        num_rows: 2748\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from ai4bharat.transliteration import XlitEngine\n",
    "\n",
    "dataset_name = \"LingoIITGN/PHINC\"\n",
    "dataset_path = \"./datasets_final/PHINC\"\n",
    "hf_marker = os.path.join(dataset_path, \"dataset_dict.json\")\n",
    "\n",
    "\n",
    "def add_transliteration_column(dataset):\n",
    "\n",
    "    engine = XlitEngine(\"hi\", beam_width=5)\n",
    "\n",
    "    def transliterate_batch(batch):\n",
    "\n",
    "        transliterated = []\n",
    "\n",
    "        for s in batch[\"hinglish\"]:\n",
    "            if s is None:\n",
    "                transliterated.append(\"\")\n",
    "            else:\n",
    "                result = engine.translit_sentence(s)\n",
    "\n",
    "                # If output is dict like {\"hi\": \"...\"}\n",
    "                if isinstance(result, dict):\n",
    "                    transliterated.append(result.get(\"hi\", \"\"))\n",
    "                else:\n",
    "                    transliterated.append(result)\n",
    "\n",
    "        return {\"hinglish_dev\": transliterated}\n",
    "\n",
    "    for split in dataset.keys():\n",
    "        if \"hinglish_dev\" not in dataset[split].column_names:\n",
    "            dataset[split] = dataset[split].map(\n",
    "                transliterate_batch,\n",
    "                batched=True,\n",
    "                batch_size=128,\n",
    "            )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Case 1: Dataset already saved locally\n",
    "if os.path.exists(hf_marker):\n",
    "    print(\"Loading dataset from local disk...\")\n",
    "    dataset = load_from_disk(dataset_path)\n",
    "\n",
    "    if \"hinglish_dev\" not in dataset[\"train\"].column_names:\n",
    "        dataset = add_transliteration_column(dataset)\n",
    "        dataset.save_to_disk(dataset_path)\n",
    "\n",
    "\n",
    "# Case 2: Dataset not present locally → download\n",
    "else:\n",
    "    print(\"Dataset not found locally. Downloading...\")\n",
    "    os.makedirs(dataset_path, exist_ok=True)\n",
    "\n",
    "    dataset = load_dataset(dataset_name)\n",
    "\n",
    "    # If only train split exists → create 70/10/20 split\n",
    "    if \"train\" in dataset and len(dataset) == 1:\n",
    "        full_dataset = dataset[\"train\"]\n",
    "\n",
    "        split_1 = full_dataset.train_test_split(test_size=0.30, seed=42)\n",
    "        temp_dataset = split_1[\"test\"]\n",
    "\n",
    "        split_2 = temp_dataset.train_test_split(test_size=2/3, seed=42)\n",
    "\n",
    "        dataset = DatasetDict({\n",
    "            \"train\": split_1[\"train\"],\n",
    "            \"validation\": split_2[\"train\"],\n",
    "            \"test\": split_2[\"test\"]\n",
    "        })\n",
    "\n",
    "    # Rename columns if needed\n",
    "    for split in dataset.keys():\n",
    "        if \"Sentence\" in dataset[split].column_names:\n",
    "            dataset[split] = dataset[split].rename_columns({\n",
    "                \"Sentence\": \"hinglish\",\n",
    "                \"English_Translation\": \"english\"\n",
    "            })\n",
    "\n",
    "    dataset = add_transliteration_column(dataset)\n",
    "\n",
    "    dataset.save_to_disk(dataset_path)\n",
    "    print(\"Saved dataset.\")\n",
    "\n",
    "\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "609ecc29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hinglish': 'Gayatri Mantra se kaam chal jaega kya @anupamamathur1 ?pic.twitter.com/Tq8EMU5P0h',\n",
       " 'english': 'would gayatri mantra be enough @anupamamathur1 ?pic.twitter.com/Tq8EMU5P0h',\n",
       " 'hinglish_dev': 'गायत्री मंत्र से काम चल जाएगा क्या @अनुपममथुर१ ?पिक.ट्विटर.कॉम/टीक्यू८ईएमयू५प०ह'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e74401",
   "metadata": {},
   "source": [
    "### HINMIX\n",
    "\n",
    "We construct a large synthetic Hinglish-English dataset by leveraging a bilingual Hindi-English corpus. Split: Train, test, valid Subsets:\n",
    "\n",
    "- Hi - Hindi in devanagiri script (Example: अमेरिकी लोग अब पहले जितनी गैस नहीं खरीदते।)\n",
    "- Hicm - Hindi sentences with codemix words substituted in English (Example: American people अब पहले जितनी gas नहीं खरीदते।)\n",
    "- Hicmrom - Hicm with romanized hindi words (Example: American people ab pahle jitni gas nahin kharidte.)\n",
    "- Hicmdvg - Hicm with transliterated english words to devangiri (Example: अमेरिकन पेओपल अब पहले जितनी गैस नहीं खरीदते।)\n",
    "- NoisyHicmrom - synthetic noise added to Hicmrom sentences to improve model robustness (Example: Aerican people ab phle jtni gas nain khridte.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3544e0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing config: lcsalign-en\n",
      "Loading existing config: lcsalign-hi\n",
      "Loading existing config: lcsalign-hicm\n",
      "Loading existing config: lcsalign-hicmdvg\n",
      "Loading existing config: lcsalign-hicmrom\n",
      "Loading existing config: lcsalign-noisyhicmrom\n",
      "All configs ready.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "import os\n",
    "\n",
    "dataset_name = \"kartikagg98/HINMIX_hi-en\"\n",
    "configs = [\n",
    "    \"lcsalign-en\",\n",
    "    \"lcsalign-hi\",\n",
    "    \"lcsalign-hicm\",\n",
    "    \"lcsalign-hicmdvg\",\n",
    "    \"lcsalign-hicmrom\",\n",
    "    \"lcsalign-noisyhicmrom\",\n",
    "]\n",
    "\n",
    "base_path = \"./datasets/HINMIX_hi-en\"\n",
    "os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "for config in configs:\n",
    "    save_path = os.path.join(base_path, config)\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"Loading existing config: {config}\")\n",
    "        ds = load_from_disk(save_path)\n",
    "    else:\n",
    "        print(f\"Downloading config: {config}\")\n",
    "        ds = load_dataset(dataset_name, config)\n",
    "        ds.save_to_disk(save_path)\n",
    "\n",
    "print(\"All configs ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d262cf19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'kripya is action ke smaarthan men uddeshya aur reasons ko prastut karen.'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "76431485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset already exists. Skipping creation.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk, Dataset, DatasetDict\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "old_base_path = \"./datasets/HINMIX_hi-en\"\n",
    "new_base_path = \"./datasets_final/HINMIX_hi-en\"\n",
    "\n",
    "# If dataset already exists, skip processing\n",
    "if os.path.exists(new_base_path):\n",
    "    print(\"Final dataset already exists. Skipping creation.\")\n",
    "else:\n",
    "    os.makedirs(new_base_path, exist_ok=True)\n",
    "\n",
    "    # Load required configs\n",
    "    hicmrom_path = os.path.join(old_base_path, \"lcsalign-hicmrom\")\n",
    "    en_path = os.path.join(old_base_path, \"lcsalign-en\")\n",
    "\n",
    "    hicmrom_ds = load_from_disk(hicmrom_path)\n",
    "    en_ds = load_from_disk(en_path)\n",
    "\n",
    "    final_dataset = {}\n",
    "\n",
    "    for split in hicmrom_ds.keys():\n",
    "\n",
    "        # Rename 'valid' to 'validation'\n",
    "        new_split_name = \"validation\" if split == \"valid\" else split\n",
    "\n",
    "        hicmrom_split = hicmrom_ds[split]\n",
    "        en_split = en_ds[split]\n",
    "\n",
    "        assert len(hicmrom_split) == len(en_split), \"Mismatch in dataset lengths\"\n",
    "\n",
    "        hinglish_data = []\n",
    "        english_data = []\n",
    "\n",
    "        for i in tqdm(range(len(hicmrom_split)), desc=f\"Processing {new_split_name}\"):\n",
    "            hinglish_data.append(hicmrom_split[i][\"text\"])\n",
    "            english_data.append(en_split[i][\"text\"])\n",
    "\n",
    "        combined = Dataset.from_dict({\n",
    "            \"hinglish\": hinglish_data,\n",
    "            \"english\": english_data\n",
    "        })\n",
    "\n",
    "        final_dataset[new_split_name] = combined\n",
    "\n",
    "    final_dataset = DatasetDict(final_dataset)\n",
    "    final_dataset.save_to_disk(new_base_path)\n",
    "\n",
    "    print(\"Final dataset saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6154e011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['hinglish', 'english'],\n",
       "        num_rows: 2507\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['hinglish', 'english'],\n",
       "        num_rows: 4200000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['hinglish', 'english'],\n",
       "        num_rows: 280\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5090b7a0",
   "metadata": {},
   "source": [
    "### English-Hinglish-TOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fcdc80e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from local disk...\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['en', 'hi_en', 'en_parse', 'hi_en_parse', 'domain', 'generated_by'],\n",
      "        num_rows: 176596\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['en', 'hi_en', 'en_parse', 'hi_en_parse', 'domain', 'generated_by'],\n",
      "        num_rows: 1390\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['en', 'hi_en', 'en_parse', 'hi_en_parse', 'domain', 'generated_by'],\n",
      "        num_rows: 6513\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "dataset_path = \"./datasets/english_hinglish_top\"\n",
    "\n",
    "if os.path.exists(dataset_path):\n",
    "    print(\"Loading dataset from local disk...\")\n",
    "    ds = load_from_disk(dataset_path)\n",
    "else:\n",
    "    print(\"Dataset not found locally. Downloading...\")\n",
    "    ds = load_dataset(\"rvv-karma/English-Hinglish-TOP\")\n",
    "    ds.save_to_disk(dataset_path)\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8bfd2115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'What is the UV rating today?',\n",
       " 'hi_en': 'Today ki UV rating kya hai?',\n",
       " 'en_parse': '[IN:GET_WEATHER What is the [SL:WEATHER_ATTRIBUTE UV rating ] [SL:DATE_TIME today ] ? ]',\n",
       " 'hi_en_parse': '[IN:GET_WEATHER [SL:DATE_TIME Today ] ki [SL:WEATHER_ATTRIBUTE UV rating ] kya hai? ]',\n",
       " 'domain': 'weather',\n",
       " 'generated_by': 'human'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d373b372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dbde0387be44a3a8e540b6a6d4a7aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/176596 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7a675b122f4b35862d08bda0271f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1390 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2f213c0e2c14d10bc061191cbb15dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6513 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9171d799eb05493b8ddf4ab783a76138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/176596 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a095b9e9d124a338f3b78ec95df2f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1390 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76bad04d3ff4413da848d911f5bb4971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/6513 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dataset saved at: ./datasets_final/english_hinglish_top\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['english', 'hinglish'],\n",
      "        num_rows: 176596\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['english', 'hinglish'],\n",
      "        num_rows: 1390\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['english', 'hinglish'],\n",
      "        num_rows: 6513\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Original dataset path\n",
    "dataset_path = \"./datasets/english_hinglish_top\"\n",
    "\n",
    "# New dataset path\n",
    "new_dataset_path = \"./datasets_final/english_hinglish_top\"\n",
    "\n",
    "# Load original dataset\n",
    "ds = load_from_disk(dataset_path)\n",
    "\n",
    "# Keep only required columns and rename them\n",
    "def transform_dataset(dataset):\n",
    "    dataset = dataset.remove_columns(\n",
    "        [col for col in dataset.column_names if col not in [\"en\", \"hi_en\"]]\n",
    "    )\n",
    "    dataset = dataset.rename_column(\"en\", \"english\")\n",
    "    dataset = dataset.rename_column(\"hi_en\", \"hinglish\")\n",
    "    return dataset\n",
    "\n",
    "ds = ds.map(lambda x: x, batched=True)\n",
    "\n",
    "# Apply transformation split-wise\n",
    "for split in ds.keys():\n",
    "    ds[split] = transform_dataset(ds[split])\n",
    "\n",
    "# Create directory if needed\n",
    "os.makedirs(\"./datasets_final\", exist_ok=True)\n",
    "\n",
    "# Save new dataset\n",
    "ds.save_to_disk(new_dataset_path)\n",
    "\n",
    "print(\"New dataset saved at:\", new_dataset_path)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff97d52",
   "metadata": {},
   "source": [
    "#### HINGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d0f42bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from local disk...\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['English', 'Hindi', 'Human-generated Hinglish', 'WAC', 'WAC rating1', 'WAC rating2', 'PAC', 'PAC rating1', 'PAC rating2'],\n",
      "        num_rows: 1976\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "dataset_path = \"./datasets/hinge\"\n",
    "\n",
    "if os.path.exists(dataset_path):\n",
    "    print(\"Loading dataset from local disk...\")\n",
    "    ds = load_from_disk(dataset_path)\n",
    "else:\n",
    "    print(\"Dataset not found locally. Downloading...\")\n",
    "    ds = load_dataset(\"LingoIITGN/HinGE\")\n",
    "    ds.save_to_disk(dataset_path)\n",
    "\n",
    "print(ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "aad757ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'English': 'It was presented to the Legislative Council in 1856 and was passed in 1860.',\n",
       " 'Hindi': 'इसे 1856 में विधायी परिषद के समक्ष प्रस्तुत किया गया और1860 में पारित किया गया।',\n",
       " 'Human-generated Hinglish': \"['Ise 1856 mein legislative council ke samaksh prastut kiya gya and 1860 mein paarit kiya gya.', 'Ise 1856 mein vidhai parishad ko present kiya and 1860 mein pass kiya.', 'It was presented to vidhayi parishad in 1856 aur 1860 me parit kiya gaya.', 'Ise 1856 me legislative council ke samaksh present kiya gaya aur 1860  me pass kiya.', '1856 me it was presented to the legislative council aur 1860 me it was passed.', '1856 me ise legislative council ke samaksh prensent kiya gaya aur 1860 me parit kiya gaya.']\",\n",
       " 'WAC': 'ise 1856 men legislative council ke samaksh prastut kiya gaya aur1860 men parit kiya gaya.',\n",
       " 'WAC rating1': 9,\n",
       " 'WAC rating2': 6,\n",
       " 'PAC': 'ise 1856 men legislative council ke samaksh prastut kiya gaya aur1860 men parit kiya gaya.',\n",
       " 'PAC rating1': 9,\n",
       " 'PAC rating2': 8}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "782f2e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85786636043c4dab94337740bdd9b9e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1976 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a44deac3eef44c228fe5c16233b9a6b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1383 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "377a36c74a574579ad5cceeb08e1ae27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/197 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5c79b1696f14999a4a511420c5d0cee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/396 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['english', 'hinglish'],\n",
      "        num_rows: 1383\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['english', 'hinglish'],\n",
      "        num_rows: 197\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['english', 'hinglish'],\n",
      "        num_rows: 396\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "from datasets import load_from_disk, DatasetDict\n",
    "\n",
    "# Paths\n",
    "dataset_path = \"./datasets/hinge\"\n",
    "new_dataset_path = \"./datasets_final/hinge\"\n",
    "\n",
    "# If final dataset already exists, load it\n",
    "if os.path.exists(new_dataset_path):\n",
    "    print(\"Loading processed dataset from disk...\")\n",
    "    dataset = load_from_disk(new_dataset_path)\n",
    "\n",
    "else:\n",
    "    print(\"Processing dataset...\")\n",
    "\n",
    "    # Load original dataset\n",
    "    ds = load_from_disk(dataset_path)\n",
    "    dataset = ds[\"train\"]\n",
    "\n",
    "    # Transform columns safely\n",
    "    def transform(example):\n",
    "        hinglish_value = example[\"Human-generated Hinglish\"]\n",
    "\n",
    "        # Convert string representation of list into actual list\n",
    "        if isinstance(hinglish_value, str):\n",
    "            try:\n",
    "                hinglish_value = ast.literal_eval(hinglish_value)\n",
    "            except (ValueError, SyntaxError):\n",
    "                hinglish_value = [hinglish_value]\n",
    "\n",
    "        # If it's a list, take the first element\n",
    "        if isinstance(hinglish_value, list):\n",
    "            hinglish_value = hinglish_value[0]\n",
    "\n",
    "        return {\n",
    "            \"english\": example[\"English\"],\n",
    "            \"hinglish\": hinglish_value\n",
    "        }\n",
    "\n",
    "    dataset = dataset.map(transform, remove_columns=dataset.column_names)\n",
    "\n",
    "    # First split: 70 percent train, 30 percent temp\n",
    "    split_1 = dataset.train_test_split(test_size=0.30, seed=42)\n",
    "\n",
    "    # Split 30 percent temp into 10 percent validation and 20 percent test\n",
    "    # Since 10/30 = 1/3, validation gets 1/3 of temp\n",
    "    split_2 = split_1[\"test\"].train_test_split(test_size=2/3, seed=42)\n",
    "\n",
    "    dataset = DatasetDict({\n",
    "        \"train\": split_1[\"train\"],\n",
    "        \"validation\": split_2[\"train\"],\n",
    "        \"test\": split_2[\"test\"]\n",
    "    })\n",
    "\n",
    "    os.makedirs(\"./datasets_final\", exist_ok=True)\n",
    "    dataset.save_to_disk(new_dataset_path)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "863fb04b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'english': 'Inevitable losses is pushing the profit in unfavourable direction.',\n",
       " 'hinglish': 'Inevitable losses labh ko pratikul direction mein dhakel rhe hai.'}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5412fcf",
   "metadata": {},
   "source": [
    "#### Lince_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7200b6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from disk...\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['english', 'hinglish'],\n",
      "        num_rows: 8060\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['english', 'hinglish'],\n",
      "        num_rows: 942\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['english'],\n",
      "        num_rows: 960\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict, load_from_disk\n",
    "import os\n",
    "\n",
    "train_valid_name = \"Huggmachas/Lince_benchmark_mt_enghinglish_train_valid\"\n",
    "test_name = \"Huggmachas/Lince_benchmark_mt_enghinglish_test\"\n",
    "\n",
    "save_path = \"./datasets/Lince_benchmark_mt\"\n",
    "\n",
    "if os.path.exists(save_path):\n",
    "    print(\"Loading dataset from disk...\")\n",
    "    ds = load_from_disk(save_path)\n",
    "else:\n",
    "    print(\"Downloading and saving dataset...\")\n",
    "    train_valid = load_dataset(train_valid_name)\n",
    "    test = load_dataset(test_name)\n",
    "\n",
    "    ds = DatasetDict({\n",
    "        \"train\": train_valid[\"train\"],\n",
    "        \"validation\": train_valid[\"dev\"],\n",
    "        \"test\": test[\"test\"]\n",
    "    })\n",
    "\n",
    "    ds.save_to_disk(save_path)\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "75b7d52b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'english': 'Batman vs Superman', 'hinglish': 'batman vs superman'}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "10255e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from disk...\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['english', 'hinglish'],\n",
      "        num_rows: 8060\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['english', 'hinglish'],\n",
      "        num_rows: 942\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['english'],\n",
      "        num_rows: 960\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict, load_from_disk\n",
    "import os\n",
    "\n",
    "train_valid_name = \"Huggmachas/Lince_benchmark_mt_enghinglish_train_valid\"\n",
    "test_name = \"Huggmachas/Lince_benchmark_mt_enghinglish_test\"\n",
    "\n",
    "save_path = \"./datasets_final/Lince_benchmark_mt\"\n",
    "\n",
    "if os.path.exists(save_path):\n",
    "    print(\"Loading dataset from disk...\")\n",
    "    ds = load_from_disk(save_path)\n",
    "else:\n",
    "    print(\"Downloading and saving dataset...\")\n",
    "    train_valid = load_dataset(train_valid_name)\n",
    "    test = load_dataset(test_name)\n",
    "\n",
    "    ds = DatasetDict({\n",
    "        \"train\": train_valid[\"train\"],\n",
    "        \"validation\": train_valid[\"dev\"],\n",
    "        \"test\": test[\"test\"]\n",
    "    })\n",
    "\n",
    "    ds.save_to_disk(save_path)\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1af535",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
